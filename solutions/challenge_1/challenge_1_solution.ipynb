{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Basics of Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this challenge you will get familar with the basic concepts of [Azure Machine Learning](https://azure.microsoft.com/en-us/services/machine-learning/). Relevant links will provided in the Notebook and help you to solve the tasks.\n",
    "\n",
    "Generally a very good source of information is the [Python SDK reference](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py) for Azure Machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Azure ML Python Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Authentication and initializing Azure Machine Learning Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step you have to authenticate against the Azure [Machine Learning Workspace](https://ml.azure.com/). This can be achieved in different ways:\n",
    "\n",
    "1. **Interactive Login Authentication:** The interactive authentication is suitable for local experimentation on your own computer.\n",
    "2. **Azure CLI Authentication:** Azure CLI authentication is suitable if you are already using Azure CLI for managing Azure resources, and want to sign in only once.\n",
    "3. **Managed Service Identity (MSI) Authentication:** The MSI authentication is suitable for automated workflows, for example as part of Azure Devops build.\n",
    "4. **Service Principal Authentication:** The Service Principal authentication is suitable for automated workflows, for example as part of Azure Devops build.\n",
    "\n",
    "For now, we will use the interactive authentication, which is the default mode when using Azure ML SDK. When you connect to your workspace using `Workspace.from_config`, you will get an interactive login dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the user you're authenticated as must have access to the subscription and resource group. If you receive an error\n",
    "```\n",
    "AuthenticationException: You don't have access to xxxxxx-xxxx-xxx-xxx-xxxxxxxxxx subscription. All the subscriptions that you have access to = ...\n",
    "```\n",
    "check that the you used correct login and entered the correct subscription ID.\n",
    "\n",
    "Alternatively, you can also specify the details of your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Alternative login method\n",
    "\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "\n",
    "ws = Workspace(subscription_id='<your-subscription-id>',\n",
    "               resource_group='<your-resource-group-name>',\n",
    "               workspace_name='<your-workspace-name>',\n",
    "               auth=interactive_auth)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we logged in, we can print the Worspace details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Print the workspace details below. See here for the workspace object reference: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Workspace name: \" + ws.name, \n",
    "      \"Azure region: \" + ws.location, \n",
    "      \"Subscription id: \" + ws.subscription_id, \n",
    "      \"Resource group: \" + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload and register data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every workspace comes with a default [datastore](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data) (and you can register more) which is backed by the Azure blob storage account associated with the workspace. We can use it to transfer data from local to the cloud, and create Dataset from it. We will now upload the Iris data to the default datastore (blob) within your workspace.\n",
    "\n",
    "By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List all datastores registered in the current workspace\n",
    "datastores = ws.datastores\n",
    "for name, datastore in datastores.items():\n",
    "    print(name, datastore.datastore_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge we will use the [default datastore](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data#get-datastores-from-your-workspace) that comes with the Azure Machine Learning Workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Retrieve the default datastore for this workspace.\n",
    "\n",
    "Hint: Same link as in the previous hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "print(datastore.name, datastore.datastore_type, datastore.account_name, datastore.container_name, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we upload data, take a minute to familiarize yourself with the folder structure of the workshop. Switch back to the home page of the jupyter environment (should be still open in a previous browser tab) and look through the folders.\n",
    "\n",
    "For instance, next to this notebook file, you will find a folder `train_dataset` which contains the training data that we will upload in the next step. Also look into the folders for the other challenges to see what there is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Upload the file `./train-dataset/iris.csv` to the target path `train-dataset/tabular/` on the default datastore.\n",
    "\n",
    "Hint: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.azure_storage_datastore.azureblobdatastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore.upload_files(files = ['./train_dataset/iris.csv'],\n",
    "                       target_path = 'train_dataset/tabular/',\n",
    "                       overwrite = True,\n",
    "                       show_progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will create and register a TabularDataset pointing to the path in the datastore. You can also create a Dataset from multiple paths. [Learn more](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets)\n",
    "\n",
    "A Dataset can reference single or multiple files in your datastores or public urls. The files can be of any format. Dataset provides you with the ability to download or mount the files to your compute. By creating a dataset, you create a reference to the data source location. The data remains in its existing location, so no extra storage cost is incurred. [Learn More](https://aka.ms/azureml/howto/createdatasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "tabular_dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, 'train_dataset/tabular/iris.csv')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Register the dataset in our workspace.\n",
    "\n",
    "Hint: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset%28class%29?view=azure-ml-py#methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_dataset = tabular_dataset.register(workspace=ws,\n",
    "                                           name='iris_tabular',\n",
    "                                           description='tabular iris training data',\n",
    "                                           create_new_version = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Get and preview 3 rows from the dataset.\n",
    "\n",
    "Hint: This is very similar to working with any other dataframe in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and preview 3 rows of the dataset\n",
    "tabular_dataset.take(3).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will register a dataset in the Azure Machine Learning Workspace as a file dataset. A file dataset can be mounted to the compute engine. When you mount a file system, you attach that file system to a directory (mount point) and make it available to the system. Because mounting load files at the time of processing, it is usually faster than download.\n",
    "Note: mounting is only available for Linux-based compute (DSVM/VM, AMLCompute, HDInsights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "file_dataset = Dataset.File.from_files(path = [(datastore, 'train_dataset/tabular/iris.csv')])\n",
    "file_dataset = file_dataset.register(workspace=ws,\n",
    "                                     name='iris_file',\n",
    "                                     description='file iris training data',\n",
    "                                     create_new_version = True)\n",
    "\n",
    "file_dataset.to_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Compute Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample, we want to train a simple scikit-learn model on a remote compute engine on Azure. To do so, we first must create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target).\n",
    "\n",
    "In this challenge, we want to use Azure ML managed compute ([AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)) for our remote training compute resource. Once this is created, you are ready to train on your remote compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TASK:** Create a machine learning compute target.\n",
    "\n",
    "Create an Azure Machine Learning Compute cluster and folow the steps one to four.\n",
    "1. Check whether the cluster with the given name already exists.\n",
    "2. Create the configuration (this step is local and only takes a second). Use the SKU `STANDARD_D2_V2` and a maximum of 4 nodes.\n",
    "3. Create the cluster (this step will take about 20 seconds)\n",
    "4. Provision the VMs to bring the cluster to the initial size. This step will take about 3-5 minutes and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell.\n",
    "\n",
    "Hint: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.computetarget?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpucluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', \n",
    "                                                           max_nodes=4,\n",
    "                                                           idle_seconds_before_scaledown=1800)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a project directory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script and any additional files your training script depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FOLDER_NAME = 'train'\n",
    "TRAIN_FILE_NAME = 'train.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join(\".\", TRAIN_FOLDER_NAME), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a training script "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will need to create your training scripts in your project folder. This will be done in the next step. In practice, you should be able to take any custom training script as is and run it with Azure ML without having to modify your code.\n",
    "\n",
    "If you would like to use Azure ML's [tracking and metrics](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#metrics) capabilities, you will have to add a small amount of Azure ML code inside your training script.\n",
    "\n",
    "In `train_iris.py`, we will log some metrics to our Azure ML run. To do so, we will access the Azure ML Run object within the script:\n",
    "\n",
    "```python\n",
    "from azureml.core.run import Run\n",
    "run = Run.get_context()\n",
    "```\n",
    "\n",
    "Further within `train_iris.py`, we log the kernel and penalty parameters, and the highest accuracy the model achieves:\n",
    "\n",
    "```python\n",
    "run.log('Kernel type', np.string(args.kernel))\n",
    "run.log('Penalty', np.float(args.penalty))\n",
    "\n",
    "run.log('Accuracy', np.float(accuracy))\n",
    "```\n",
    "\n",
    "These run metrics will become particularly important when we begin hyperparameter tuning our model in the \"Tune model hyperparameters\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: The training script below misses to log a few of the metrics. Find the `???` and complete the script\n",
    "\n",
    "Hint: Be careful when retrieving the metrics. The default is `average='binary'` and might not be the right fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $TRAIN_FOLDER_NAME/$TRAIN_FILE_NAME\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from azureml.core import Dataset, Run\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--kernel', type=str, default='rbf', help='Kernel type to be used in the algorithm')\n",
    "    parser.add_argument('--penalty', type=float, default=1.0, help='Penalty parameter of the error term')\n",
    "    parser.add_argument('--modelname', type=str, default='model.pkl', help='Name of the model file')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    run = Run.get_context()\n",
    "    run.log('Kernel type', np.str(args.kernel))\n",
    "    run.log('Penalty', np.float(args.penalty))\n",
    "\n",
    "    # loading the iris dataset\n",
    "    dataset = run.input_datasets['iris']\n",
    "    try:\n",
    "        df = dataset.to_pandas_dataframe()\n",
    "    except:\n",
    "        print('Dataset path: ', str(dataset))\n",
    "        df = pd.read_csv(os.path.join(dataset))\n",
    "    \n",
    "    # split dataset\n",
    "    x_col = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "    y_col = ['species']\n",
    "    x_df = df.loc[:, x_col]\n",
    "    y_df = df.loc[:, y_col]\n",
    "    \n",
    "    #dividing X,y into train and test data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=223)\n",
    "    data = {'train': {'X': x_train, 'y': y_train},\n",
    "            'test': {'X': x_test, 'y': y_test}}\n",
    "    \n",
    "    # training a SVM classifier\n",
    "    svm_model = SVC(kernel=args.kernel, C=args.penalty, gamma='scale').fit(data['train']['X'], data['train']['y'])\n",
    "    svm_predictions = svm_model.predict(data['test']['X'])\n",
    "\n",
    "    # model accuracy for X_test\n",
    "    accuracy = svm_model.score(data['test']['X'], data['test']['y'])\n",
    "    print('Accuracy of SVM classifier on test set: {:.2f}'.format(accuracy))\n",
    "    run.log('Accuracy', np.float(accuracy))\n",
    "    \n",
    "    # precision for X_test\n",
    "    precision = precision_score(svm_predictions, data[\"test\"][\"y\"], average='weighted')\n",
    "    print('Precision of SVM classifier on test set: {:.2f}'.format(precision))\n",
    "    run.log('precision', precision)\n",
    "    \n",
    "    # recall for X_test\n",
    "    recall = recall_score(svm_predictions, data[\"test\"][\"y\"], average='weighted')\n",
    "    print('Recall of SVM classifier on test set: {:.2f}'.format(recall))\n",
    "    run.log('recall', recall)\n",
    "    \n",
    "    # f1-score for X_test\n",
    "    f1 = f1_score(svm_predictions, data[\"test\"][\"y\"], average='weighted')\n",
    "    print('F1-Score of SVM classifier on test set: {:.2f}'.format(f1))\n",
    "    run.log('f1-score', f1)\n",
    "    \n",
    "    # creating a confusion matrix\n",
    "    cm = confusion_matrix(y_test, svm_predictions)\n",
    "    \n",
    "    cm_json =   {\n",
    "       \"schema_type\": \"confusion_matrix\",\n",
    "       \"schema_version\": \"v1\",\n",
    "       \"data\": {\n",
    "           \"class_labels\": x_col,\n",
    "           \"matrix\": cm.tolist()\n",
    "       }\n",
    "    }\n",
    "    print(cm_json)\n",
    "    run.log_confusion_matrix('confusion matrix', cm_json)\n",
    "    \n",
    "    # files saved in the \"outputs\" folder are automatically uploaded into run history\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    joblib.dump(svm_model, 'outputs/' + args.modelname)\n",
    "    run.log('Model Name', np.str(args.modelname))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An *Experiment* is a logical container in an Azure ML Workspace that represents a collection of trials (individual model runs). It hosts run records which can include run metrics and output artifacts from your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Fill in the missing values below to create a new experiment in your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name='ch1-sklearn_sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An estimator object is used to submit the run. Azure Machine Learning has pre-configured estimators for common machine learning frameworks, as well as generic Estimator. Create a generic estimator for by specifying\n",
    "\n",
    "- The name of the estimator object, est\n",
    "- The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution.\n",
    "- The training script name, train_titanic.py\n",
    "- The input Dataset for training\n",
    "- The compute target. In this case you will use the AmlCompute you created\n",
    "- The environment definition for the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Complete the estimator creation below.\n",
    "\n",
    "Hint: https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.estimator.estimator?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--kernel': 'linear',\n",
    "    '--penalty': 1.0\n",
    "}\n",
    "\n",
    "est = Estimator(source_directory=TRAIN_FOLDER_NAME, \n",
    "                entry_script=TRAIN_FILE_NAME,\n",
    "                script_params=script_params,\n",
    "                inputs=[tabular_dataset.as_named_input('iris')],\n",
    "                compute_target=compute_target,\n",
    "                pip_packages=['azureml-dataprep[pandas,fuse]', 'scikit-learn', 'pandas']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Alternatives\n",
    "from azureml.train.sklearn import SKLearn\n",
    "\n",
    "# with mounting the dataset\n",
    "est = Estimator(source_directory=TRAIN_FOLDER_NAME, \n",
    "                entry_script=TRAIN_FILE_NAME,\n",
    "                script_params=script_params,\n",
    "                inputs=[file_dataset.as_named_input('iris').as_mount('tmp/dataset')],\n",
    "                compute_target=compute_target,\n",
    "                pip_packages=['azureml-dataprep[pandas,fuse]', 'scikit-learn', 'pandas'])\n",
    "\n",
    "# with specific estimator for scikit-learn\n",
    "est = SKLearn(source_directory=TRAIN_FOLDER_NAME,\n",
    "              script_params=script_params,\n",
    "              compute_target=compute_target,\n",
    "              entry_script=TRAIN_FILE_NAME,\n",
    "              pip_packages=['azureml-dataprep[pandas,fuse]', 'pandas'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Submit the job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the estimator to the Azure ML experiment to kick off the execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Submit the experiment as a new run. \n",
    "\n",
    "While the experiment is running (after the Docker image was built and pushed), you can take a look at the compute target in the AzureML UI. Initially it will have 0 nodes running. To execute the experiment, you will see that the cluster is being scaled up to 1 node.\n",
    "\n",
    "Hint: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment%28class%29?view=azure-ml-py#methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output=True, wait_post_processing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cancel a run, you can call `run.cancel()`. However, if you want to do that, you need to interrupt the jupyter kernel first, since `run.wait_for_completion()` will only return once the run is completed or cancelled (doing this will not cancel the run execution itself).\n",
    "Otherwise you can also cancel the run from the AzureML UI. Click on the link that is printed in the first output line of the above command to open the portal. There you will find a Cancel button as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the run is finished, we first get its status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a model trained on a remote cluster. Retrieve all the metrics logged during the run, including the accuracy of the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Retrieve the metrics of the run.\n",
    "\n",
    "Hint: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run%28class%29?view=azure-ml-py#methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tune model hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen how to do a simple Scikit-learn training run using the SDK, let's see if we can further improve the accuracy of our model. We can optimize our model's hyperparameters using Azure Machine Learning's hyperparameter tuning capabilities.\n",
    "\n",
    "First, we will define the hyperparameter space to sweep over. Let's tune the `kernel` and `penalty` parameters. In this example we will use random sampling to try different configuration sets of hyperparameters to maximize our primary metric, `Accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice, uniform\n",
    "from azureml.train.hyperdrive.policy import MedianStoppingPolicy\n",
    "\n",
    "param_sampling = RandomParameterSampling({\n",
    "    \"--kernel\": choice('linear', 'rbf', 'poly', 'sigmoid'),\n",
    "    \"--penalty\": uniform(0.5, 1.5)\n",
    "    })\n",
    "\n",
    "hyperdrive_run_config = HyperDriveConfig(estimator=est,\n",
    "                                         hyperparameter_sampling=param_sampling, \n",
    "                                         primary_metric_name='Accuracy',\n",
    "                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                         max_total_runs=6,\n",
    "                                         max_concurrent_runs=4,\n",
    "                                         policy=MedianStoppingPolicy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lauch the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Submit the hyperdrive run\n",
    "\n",
    "Hint: Is is very similar to the experiment submission before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run = exp.submit(hyperdrive_run_config)\n",
    "hyperdrive_run.wait_for_completion(show_output=True, wait_post_processing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: While the hyperdrive is running, you can use the time to take a look at the newly created Azure Container Registry in the Azure Portal. It was created when you submitted the first remote run.\n",
    "During the experiment submission, after the Docker image was built, it was pushed up into that registry. \n",
    "\n",
    "Try to find the pushed image in the registry and take a look at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times, finding the best hyperparameter values for your model can be an iterative process, needing multiple tuning runs that learn from previous hyperparameter tuning runs. Reusing knowledge from these previous runs will accelerate the hyperparameter tuning process, thereby reducing the cost of tuning the model and will potentially improve the primary metric of the resulting model. When warm starting a hyperparameter tuning experiment with Bayesian sampling, trials from the previous run will be used as prior knowledge to intelligently pick new samples, so as to improve the primary metric. Additionally, when using Random or Grid sampling, any early termination decisions will leverage metrics from the previous runs to determine poorly performing training runs. \n",
    "\n",
    "Azure Machine Learning allows you to warm start your hyperparameter tuning run by leveraging knowledge from up to 5 previously completed hyperparameter tuning parent runs. \n",
    "\n",
    "Additionally, there might be occasions when individual training runs of a hyperparameter tuning experiment are cancelled due to budget constraints or fail due to other reasons. It is now possible to resume such individual training runs from the last checkpoint (assuming your training script handles checkpoints). Resuming an individual training run will use the same hyperparameter configuration and mount the storage used for that run. The training script should accept the \"--resume-from\" argument, which contains the checkpoint or model files from which to resume the training run. You can also resume individual runs as part of an experiment that spends additional budget on hyperparameter tuning. Any additional budget, after resuming the specified training runs is used for exploring additional configurations.\n",
    "\n",
    "For more information on warm starting and resuming hyperparameter tuning runs, please refer to the [Hyperparameter Tuning for Azure Machine Learning documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all jobs finish, we can find out the one that has the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Get the best run from the hyperdrive experiment\n",
    "\n",
    "Hint: https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriverun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "print(best_run.get_details()['runDefinition']['arguments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Register model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in the training script wrote the file `model.pkl` in a directory named `outputs` in the VM of the cluster where the job is executed. `outputs` is a special directory in that all content in this  directory is automatically uploaded to your workspace.  This content appears in the run record in the experiment under your workspace. Hence, the model file is now also available in your workspace.\n",
    "\n",
    "You can see files associated with that run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Get all the file names associated with the best run.\n",
    "\n",
    "Hint: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run%28class%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.get_file_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the model in the workspace so that you (or other collaborators) can later query, examine, and deploy this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Fill in the missing values below to register the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "model = best_run.register_model(model_name='sample-model',\n",
    "                                model_path='outputs/model.pkl',\n",
    "                                model_framework=Model.Framework.SCIKITLEARN,\n",
    "                                model_framework_version='0.22.1',\n",
    "                                datasets=[('Training dataset', tabular_dataset)],\n",
    "                                resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n",
    "                                description='SVC classification for iris dataset.',\n",
    "                                tags={'area': 'iris', 'type': 'svc'})\n",
    "\n",
    "print('Name: ' + model.name, 'ID: ' + model.id, 'Version: ' + str(model.version), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your model is ready for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No-code model deployment is currently in preview and supports various frameworks and model types including Tensorflow SavedModel format, ONNX models and Scikit-learn models. No code model deployment is supported for all built-in scikit-learn model types.\n",
    "\n",
    "The deployment will take a few minutes and will take place on an Azure Container Instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Fill in the missing values to deploy the model as a no-code webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "dt = datetime.datetime.now().strftime(\"d%H%M%S\")\n",
    "\n",
    "service_no_code = Model.deploy(workspace=ws,\n",
    "                               name='ch1-service-nocode-' + dt,\n",
    "                               models=[model],\n",
    "                               overwrite=True)\n",
    "service_no_code.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If deployment fails, then retry with:\n",
    "#service_no_code.update(models=[model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Get the logs for the web service\n",
    "\n",
    "Hint: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice%28class%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "service_no_code.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert this Webservice object into a JSON serialized dictionary, which lists all the details of the webservice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "service_no_code.serialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is an example of a Python client that can be used with the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Fill in the missing part to execute the request against the web service\n",
    "\n",
    "Hint: Look in the same class as for the previous hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Two sets of data to score, so we get two results back.\n",
    "data = {'data':\n",
    "        [\n",
    "            [ 1,2,3,4 ],\n",
    "            [ 10,9,8,7 ]\n",
    "        ]\n",
    "        }\n",
    "# Convert to JSON string.\n",
    "input_data = json.dumps(data)\n",
    "\n",
    "# Make the request and display the response.\n",
    "resp = service_no_code.run(input_data)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Package model to custom Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you might want to create a Docker image without directly deploying the model (if, for example, you plan to deploy to Azure App Service). Or you might want to download the image and run it on a local Docker installation. You might even want to download the files used to build the image, inspect them, modify them, and build the image manually.\n",
    "\n",
    "Model packaging enables you to do these things. It packages all the assets needed to host a model as a web service and allows you to download either a fully built Docker image or the files needed to build one. There are two ways to use model packaging:\n",
    "\n",
    "- Download a packaged model: Download a Docker image that contains the model and other files needed to host it as a web service.\n",
    "\n",
    "- Generate a Dockerfile: Download the Dockerfile, model, entry script, and other assets needed to build a Docker image. You can then inspect the files or make changes before you build the image locally.\n",
    "\n",
    "Both packages can be used to get a local Docker image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a custom scoring script. Usually those are referred to as something like `score.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_FOLDER_NAME = 'deployment'\n",
    "SCORE_FILE_NAME = 'score.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join(\".\", SCORE_FOLDER_NAME), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to read through the script and the comments to understand the logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $SCORE_FOLDER_NAME/$SCORE_FILE_NAME\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from azureml.monitoring import ModelDataCollector\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n",
    "\n",
    "# The init() method is called once, when the web service starts up.\n",
    "# Typically you would deserialize the model file, as shown here using joblib,\n",
    "# and store it in a global variable so your run() method can access it later.\n",
    "def init():\n",
    "    global model\n",
    "    global inputs_dc, prediction_dc\n",
    "    # The AZUREML_MODEL_DIR environment variable indicates\n",
    "    # a directory containing the model file you registered.\n",
    "    model_filename = 'model.pkl'\n",
    "    model_path = os.path.join(os.environ['AZUREML_MODEL_DIR'], model_filename)\n",
    "    model = joblib.load(model_path)\n",
    "    inputs_dc = ModelDataCollector(\"sample-model\", designation=\"inputs\", feature_names=[\"feat1\", \"feat2\", \"feat3\", \"feat4\"])\n",
    "    prediction_dc = ModelDataCollector(\"sample-model\", designation=\"predictions\", feature_names=[\"prediction\"])\n",
    "\n",
    "# The run() method is called each time a request is made to the scoring API.\n",
    "#\n",
    "# Shown here are the optional input_schema and output_schema decorators\n",
    "# from the inference-schema pip package. Using these decorators on your\n",
    "# run() method parses and validates the incoming payload against\n",
    "# the example input you provide here. This will also generate a Swagger\n",
    "# API document for your web service.\n",
    "@input_schema('data', NumpyParameterType(np.array([[0.1, 1.2, 2.3, 3.4]])))\n",
    "@output_schema(StandardPythonParameterType({'predict': [['Iris-virginica']]}))\n",
    "def run(data):\n",
    "    # Use the model object loaded by init().\n",
    "    result = model.predict(data)\n",
    "    inputs_dc.collect(data) #this call is saving our input data into Azure Blob\n",
    "    prediction_dc.collect(result) #this call is saving our input data into Azure Blob\n",
    "\n",
    "    # You can return any JSON-serializable object.\n",
    "    return { 'predict': result.tolist() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the scoring script, we also need to specify an Environment. This contains all the required pip or conda packages, that the `score.py` script needs to run properly. Those dependencies will be installed during the docker build step later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "environment = Environment(name='ch1-service-environment')\n",
    "environment.python.conda_dependencies = CondaDependencies.create(pip_packages=[\n",
    "    'azureml-defaults',\n",
    "    'joblib',\n",
    "    'numpy',\n",
    "    'scikit-learn',\n",
    "    'inference-schema',\n",
    "    'inference-schema[numpy-support]',\n",
    "    'azureml-monitoring'\n",
    "])\n",
    "environment.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To bundle with scoring script and the Environment, we now create an Inference config:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Fill in the missing values to create the inference config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=SCORE_FILE_NAME,\n",
    "                                   source_directory=SCORE_FOLDER_NAME,\n",
    "                                   description='SVC classification for iris dataset.',\n",
    "                                   environment=environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, we package the model. This means to build and push the docker image into our private container registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Fill in the missing values to package the model\n",
    "\n",
    "Hint: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "package = Model.package(ws, [model], inference_config)\n",
    "package.wait_for_creation(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acr = package.get_container_registry()\n",
    "print(\"Address:\", acr.address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the package output to the local machine. This can only be used with a Docker image package. Docker must be running on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package.pull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Deployment with inference_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Fill in the missing values to deploy the model with the inference config from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime.now().strftime(\"d%H%M%S\")\n",
    "\n",
    "service_custom = Model.deploy(workspace=ws,\n",
    "                       name='ch1-service-custom-' + dt,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config)\n",
    "service_custom.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If deployment fails, then retry with:\n",
    "#service_custom.update(models=[model], inference_config=inference_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_custom.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_custom.serialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Two sets of data to score, so we get two results back.\n",
    "data = {'data':\n",
    "        [\n",
    "            [ 1,2,3,4 ],\n",
    "            [ 10,9,8,7 ]\n",
    "        ]\n",
    "        }\n",
    "# Convert to JSON string.\n",
    "input_data = json.dumps(data)\n",
    "\n",
    "# Make the request and display the response.\n",
    "resp = service_custom.run(input_data)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#service_custom.update(enable_app_insights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Cleanup\n",
    "\n",
    "To clean up (only yet if you don't want to do the bonus challenges below!), we now delete the two created web service endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_no_code.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_custom.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Bonus\n",
    "In case you still have time left, here are a few more optional things you can try to implement in the notebook above:\n",
    "\n",
    "- Create a new blob storage account in the Azure Portal. Then, register that new account as a new datastore in your AzureML workspace. [Hint](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data)\n",
    "- In the web service test (the part where you call the web services with sample data using `service.run(input_data)`), try to use a standard HTTP request instead to call the service. Hint: Use the python `requests` package and the URL of the web service (`service.scoring_uri`).\n",
    "- Find the cell above with the line `service_custom.update(enable_app_insights=True)`. Find out what it will do. Once you have executed it, look at the outputs it creates in the Azure Application Insights service. [Hint](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-enable-app-insights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
