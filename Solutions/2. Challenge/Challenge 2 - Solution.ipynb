{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will train a PyTorch model on the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset using distributed training via Nccl/Gloo across a GPU cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Azure ML Python Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Authentication and initializing Azure Machine Learning Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step you have to authenticate against the Azure [Machine Learning Workspace](https://ml.azure.com/). This can be achieved in different ways:\n",
    "\n",
    "1. **Interactive Login Authentication:** The interactive authentication is suitable for local experimentation on your own computer.\n",
    "2. **Azure CLI Authentication:** Azure CLI authentication is suitable if you are already using Azure CLI for managing Azure resources, and want to sign in only once.\n",
    "3. **Managed Service Identity (MSI) Authentication:** The MSI authentication is suitable for automated workflows, for example as part of Azure Devops build.\n",
    "4. **Service Principal Authentication:** The Service Principal authentication is suitable for automated workflows, for example as part of Azure Devops build.\n",
    "\n",
    "For now, we will use the interactive authentication, which is the default mode when using Azure ML SDK. When you connect to your workspace using `Workspace.from_config`, you will get an interactive login dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Workspace name: \" + ws.name, \n",
    "      \"Azure region: \" + ws.location, \n",
    "      \"Subscription id: \" + ws.subscription_id, \n",
    "      \"Resource group: \" + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Compute Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample, we want to train a simple scikit-learn model on a remote compute engine on Azure. To do so, we first must create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target).\n",
    "\n",
    "In this challenge, we want to use Azure ML managed compute ([AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)) for our remote training compute resource. Once this is created, you are ready to train on your remote compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task:** Create a machine learning compute target.\n",
    "\n",
    "Create an Azure Machine Learning Compute cluster and folow the steps one to four.\n",
    "1. Check whether the cluster with the given name already exists.\n",
    "2. Create the configuration (this step is local and only takes a second). Use the SKU `STANDARD_D2_V2` and a maximum of 4 nodes.\n",
    "3. Create the cluster (this step will take about 20 seconds)\n",
    "4. Provision the VMs to bring the cluster to the initial size. This step will take about 3-5 minutes and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpucluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a project directory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script and any additional files your training script depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FOLDER_NAME = 'train'\n",
    "TRAIN_FILE_NAME = 'train.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join(\".\", TRAIN_FOLDER_NAME), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a training script "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will need to create your training scripts in your project folder. This will be done in the next step. In practice, you should be able to take any custom training script as is and run it with Azure ML without having to modify your code.\n",
    "\n",
    "If you would like to use Azure ML's [tracking and metrics](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#metrics) capabilities, you will have to add a small amount of Azure ML code inside your training script.\n",
    "\n",
    "In `train_iris.py`, we will log some metrics to our Azure ML run. To do so, we will access the Azure ML Run object within the script:\n",
    "\n",
    "```python\n",
    "from azureml.core.run import Run\n",
    "run = Run.get_context()\n",
    "```\n",
    "\n",
    "Further within `train_iris.py`, we log the kernel and penalty parameters, and the highest accuracy the model achieves:\n",
    "\n",
    "```python\n",
    "run.log('Kernel type', np.string(args.kernel))\n",
    "run.log('Penalty', np.float(args.penalty))\n",
    "\n",
    "run.log('Accuracy', np.float(accuracy))\n",
    "```\n",
    "\n",
    "These run metrics will become particularly important when we begin hyperparameter tuning our model in the \"Tune model hyperparameters\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $TRAIN_FOLDER_NAME/$TRAIN_FILE_NAME\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import onnx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.models as models\n",
    "\n",
    "from azureml.core.run import Run\n",
    "# get the Azure ML run object\n",
    "run = Run.get_context()\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('--world-size', default=1, type=int,\n",
    "                    help='number of distributed processes')\n",
    "parser.add_argument('--dist-url', type=str,\n",
    "                    help='url used to set up distributed training')\n",
    "parser.add_argument('--dist-backend', default='nccl', type=str,\n",
    "                    help='distributed backend')\n",
    "parser.add_argument('--rank', default=-1, type=int,\n",
    "                    help='rank of the worker')\n",
    "\n",
    "best_prec1 = 0\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.distributed = args.world_size >= 2\n",
    "\n",
    "if args.distributed:\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                            world_size=args.world_size, rank=args.rank)\n",
    "\n",
    "train_dataset = datasets.MNIST('data-%d' % args.rank, train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]))\n",
    "\n",
    "if args.distributed:\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "else:\n",
    "    train_sampler = None\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size, shuffle=(train_sampler is None),\n",
    "    num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim = 1)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "if not args.distributed:\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "else:\n",
    "    model.cuda()\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        try:\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1[0], input.size(0))\n",
    "            top5.update(prec5[0], input.size(0))\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                run.log(\"loss\", losses.avg)\n",
    "                run.log(\"prec@1\", \"{0:.3f}\".format(top1.avg))\n",
    "                run.log(\"prec@5\", \"{0:.3f}\".format(top5.avg))\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                      'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
    "                                                                      batch_time=batch_time, data_time=data_time,\n",
    "                                                                      loss=losses, top1=top1, top5=top5))\n",
    "        except:\n",
    "            import sys\n",
    "            print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "    \n",
    "# Create 'outputs' folder\n",
    "os.makedirs(os.path.join(\".\", 'outputs'), exist_ok=True)\n",
    "\n",
    "# Save model as pt file\n",
    "torch.save(model, os.path.join('outputs', 'model.pt'))\n",
    "\n",
    "# Save model as ONNX file\n",
    "dummy_input = torch.randn(1, 1, 28, 28, device='cuda')\n",
    "torch.onnx.export(model,\n",
    "                  dummy_input,\n",
    "                  os.path.join('outputs', 'model.onnx'),\n",
    "                  export_params=True,\n",
    "                  opset_version=10,\n",
    "                  do_constant_folding=True,\n",
    "                  verbose=True,\n",
    "                  input_names=[ \"image\" ],\n",
    "                  output_names=[ \"log_softmax_pred\" ])\n",
    "\n",
    "# Check ONNX model\n",
    "onnx_model = onnx.load(os.path.join('outputs', 'model.onnx'))\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An *Experiment* is a logical container in an Azure ML Workspace that represents a collection of trials (individual model runs). It hosts run records which can include run metrics and output artifacts from your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name='pytorch_sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An estimator object is used to submit the run. Azure Machine Learning has pre-configured estimators for common machine learning frameworks, as well as generic Estimator. Create a generic estimator for by specifying\n",
    "\n",
    "- The name of the estimator object, est\n",
    "- The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution.\n",
    "- The training script name, train_titanic.py\n",
    "- The input Dataset for training\n",
    "- The compute target. In this case you will use the AmlCompute you created\n",
    "- The environment definition for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import PyTorch, Nccl\n",
    "\n",
    "script_params = {'--dist-backend' : 'nccl',\n",
    "                 '--dist-url': '$AZ_BATCHAI_PYTORCH_INIT_METHOD',\n",
    "                 '--rank': '$AZ_BATCHAI_TASK_INDEX',\n",
    "                 '--world-size': 2,\n",
    "                 '--epochs': 1}\n",
    "\n",
    "est = PyTorch(source_directory=TRAIN_FOLDER_NAME,\n",
    "              entry_script=TRAIN_FILE_NAME,\n",
    "              script_params=script_params,\n",
    "              compute_target=compute_target,\n",
    "              node_count=2,\n",
    "              distributed_training=Nccl(),\n",
    "              use_gpu=True,\n",
    "              framework_version='1.3',\n",
    "              pip_packages=['onnx', 'Pillow==6.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, `script_params` uses Azure ML generated `AZ_BATCHAI_PYTORCH_INIT_METHOD` for shared file-system initialization and `AZ_BATCHAI_TASK_INDEX` as rank of each worker process.\n",
    "The above code specifies that we will run our training script on `2` nodes, with one worker per node. In order to execute a distributed run using Nccl, you must provide the argument `distributed_training=Nccl()`. Using this estimator with these settings, PyTorch and dependencies will be installed for you. However, if your script also uses other packages, make sure to install them via the `PyTorch` constructor's `pip_packages` or `conda_packages` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Alternative\n",
    "from azureml.train.dnn import PyTorch, Gloo\n",
    "\n",
    "script_params = {'--dist-backend' : 'gloo',\n",
    "                 '--dist-url': '$AZ_BATCHAI_PYTORCH_INIT_METHOD',\n",
    "                 '--rank': '$AZ_BATCHAI_TASK_INDEX',\n",
    "                 '--world-size': 2}\n",
    "\n",
    "est = PyTorch(source_directory=TRAIN_FOLDER_NAME,\n",
    "                    entry_script=TRAIN_FILE_NAME,\n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target,\n",
    "                    node_count=2,\n",
    "                    distributed_training=Gloo(),\n",
    "                    use_gpu=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, `script_params` uses Azure ML generated `AZ_BATCHAI_PYTORCH_INIT_METHOD` for shared file-system initialization and `AZ_BATCHAI_TASK_INDEX` as rank of each worker process.\n",
    "The above code specifies that we will run our training script on `2` nodes, with one worker per node. In order to execute a distributed run using Gloo, you must provide the argument `distributed_training=Gloo()`. Using this estimator with these settings, PyTorch and dependencies will be installed for you. However, if your script also uses other packages, make sure to install them via the `PyTorch` constructor's `pip_packages` or `conda_packages` parameters.\n",
    "\n",
    "Once you create the estimaotr you can follow the submit steps as shown above to submit a PyTorch run with `Gloo` backend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Submit the job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the estimator to the Azure ML experiment to kick off the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for horovod (setup.py): started\n",
      "  Building wheel for horovod (setup.py): finished with status 'error'\n",
      "\u001b[91m  ERROR: Command errored out with exit status 1:\n",
      "   command: /azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-d0wy1r6w/horovod/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-d0wy1r6w/horovod/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-vnj9vh7n --python-tag cp36\n",
      "       cwd: /tmp/pip-install-d0wy1r6w/horovod/\n",
      "  Complete output (169 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-3.6\n",
      "  creating build/lib.linux-x86_64-3.6/horovod\n",
      "  copying horovod/__init__.py -> build/lib.linux-x86_64-3.6/horovod\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/keras\n",
      "  copying horovod/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/keras\n",
      "  copying horovod/keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/keras\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/_keras\n",
      "  copying horovod/_keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/_keras\n",
      "  copying horovod/_keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/_keras\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/common\n",
      "  copying horovod/common/util.py -> build/lib.linux-x86_64-3.6/horovod/common\n",
      "  copying horovod/common/basics.py -> build/lib.linux-x86_64-3.6/horovod/common\n",
      "  copying horovod/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/common\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run\n",
      "  copying horovod/run/mpi_run.py -> build/lib.linux-x86_64-3.6/horovod/run\n",
      "  copying horovod/run/task_fn.py -> build/lib.linux-x86_64-3.6/horovod/run\n",
      "  copying horovod/run/gloo_run.py -> build/lib.linux-x86_64-3.6/horovod/run\n",
      "  copying horovod/run/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run\n",
      "  copying horovod/run/run.py -> build/lib.linux-x86_64-3.6/horovod/run\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/spark\n",
      "  copying horovod/spark/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/tensorflow\n",
      "  copying horovod/tensorflow/util.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\n",
      "  copying horovod/tensorflow/__init__.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\n",
      "  copying horovod/tensorflow/compression.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\n",
      "  copying horovod/tensorflow/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/torch\n",
      "  copying horovod/torch/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch\n",
      "  copying horovod/torch/compression.py -> build/lib.linux-x86_64-3.6/horovod/torch\n",
      "  copying horovod/torch/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/torch\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/mxnet\n",
      "  copying horovod/mxnet/__init__.py -> build/lib.linux-x86_64-3.6/horovod/mxnet\n",
      "  copying horovod/mxnet/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/mxnet\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run/util\n",
      "  copying horovod/run/util/cache.py -> build/lib.linux-x86_64-3.6/horovod/run/util\n",
      "  copying horovod/run/util/network.py -> build/lib.linux-x86_64-3.6/horovod/run/util\n",
      "  copying horovod/run/util/threads.py -> build/lib.linux-x86_64-3.6/horovod/run/util\n",
      "  copying horovod/run/util/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/util\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run/common\n",
      "  copying horovod/run/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run/task\n",
      "  copying horovod/run/task/task_service.py -> build/lib.linux-x86_64-3.6/horovod/run/task\n",
      "  copying horovod/run/task/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/task\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run/rendezvous\n",
      "  copying horovod/run/rendezvous/http_server.py -> build/lib.linux-x86_64-3.6/horovod/run/rendezvous\n",
      "  copying horovod/run/rendezvous/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/rendezvous\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run/driver\n",
      "  copying horovod/run/driver/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/run/driver\n",
      "  copying horovod/run/driver/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/driver\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/env.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/network.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/safe_shell_exec.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/secret.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/codec.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/timeout.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/config_parser.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/host_hash.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/settings.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run/common/service\n",
      "  copying horovod/run/common/service/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service\n",
      "  copying horovod/run/common/service/task_service.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service\n",
      "  copying horovod/run/common/service/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/spark/task\n",
      "  copying horovod/spark/task/task_service.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\n",
      "  copying horovod/spark/task/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\n",
      "  copying horovod/spark/task/mpirun_exec_fn.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/spark/driver\n",
      "  copying horovod/spark/driver/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\n",
      "  copying horovod/spark/driver/mpirun_rsh.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\n",
      "  copying horovod/spark/driver/job_id.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\n",
      "  copying horovod/spark/driver/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\n",
      "  copying horovod/tensorflow/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\n",
      "  copying horovod/tensorflow/keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib_impl\n",
      "  copying horovod/torch/mpi_lib_impl/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib_impl\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib\n",
      "  copying horovod/torch/mpi_lib/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib\n",
      "  running build_ext\n",
      "  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -Wall -I/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/include/python3.6m -c build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.cc -o build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.o\n",
      "  cc1plus: warning: command line option â€˜-Wstrict-prototypesâ€™ is valid for C/ObjC but not for C++\n",
      "  gcc -pthread -shared -L/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib -Wl,-rpath=/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib,--no-as-needed -L/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib -Wl,-rpath=/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib,--no-as-needed build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.o -L/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib -o build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.so\n",
      "  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/include/python3.6m -c build/temp.linux-x86_64-3.6/test_compile/test_link_flags.cc -o build/temp.linux-x86_64-3.6/test_compile/test_link_flags.o\n",
      "  cc1plus: warning: command line option â€˜-Wstrict-prototypesâ€™ is valid for C/ObjC but not for C++\n",
      "  gcc -pthread -shared -L/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib -Wl,-rpath=/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib,--no-as-needed -L/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib -Wl,-rpath=/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib,--no-as-needed -Wl,--version-script=horovod.lds build/temp.linux-x86_64-3.6/test_compile/test_link_flags.o -L/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib -o build/temp.linux-x86_64-3.6/test_compile/test_link_flags.so\n",
      "  INFO: Cannot find CMake, will skip compiling Horovod with Gloo.\n",
      "  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -Wall -I/usr/local/cuda/include -I/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/include/python3.6m -c build/temp.linux-x86_64-3.6/test_compile/test_cuda.cc -o build/temp.linux-x86_64-3.6/test_compile/test_cuda.o\n",
      "  cc1plus: warning: command line option â€˜-Wstrict-prototypesâ€™ is valid for C/ObjC but not for C++\n",
      "  gcc -pthread -shared -L/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib -Wl,-rpath=/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib,--no-as-needed -L/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib -Wl,-rpath=/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib,--no-as-needed build/temp.linux-x86_64-3.6/test_compile/test_cuda.o -L/usr/local/cuda/lib -L/usr/local/cuda/lib64 -L/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib -lcudart -o build/temp.linux-x86_64-3.6/test_compile/test_cuda.so\n",
      "  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -Wall -I/usr/local/cuda/include -I/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/include/python3.6m -c build/temp.linux-x86_64-3.6/test_compile/test_nccl.cc -o build/temp.linux-x86_64-3.6/test_compile/test_nccl.o\n",
      "  cc1plus: warning: command line option â€˜-Wstrict-prototypesâ€™ is valid for C/ObjC but not for C++\n",
      "  gcc -pthread -shared -L/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib -Wl,-rpath=/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib,--no-as-needed -L/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib -Wl,-rpath=/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib,--no-as-needed build/temp.linux-x86_64-3.6/test_compile/test_nccl.o -L/usr/local/cuda/lib -L/usr/local/cuda/lib64 -L/azureml-envs/azureml_5790192de1b42dcd9b8599e089445113/lib -lnccl_static -o build/temp.linux-x86_64-3.6/test_compile/test_nccl.so\n",
      "  INFO: Unable to build TensorFlow plugin, will skip it.\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-d0wy1r6w/horovod/setup.py\", line 72, in check_tf_version\n",
      "      import tensorflow as tf\n",
      "  ModuleNotFoundError: No module named 'tensorflow'\n",
      "  \n",
      "  During handling of the above exception, another exception occurred:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-d0wy1r6w/horovod/setup.py\", line 1375, in build_extensions\n",
      "      build_tf_extension(self, options)\n",
      "    File \"/tmp/pip-install-d0wy1r6w/horovod/setup.py\", line 851, in build_tf_extension\n",
      "      check_tf_version()\n",
      "    File \"/tmp/pip-install-d0wy1r6w/horovod/setup.py\", line 79, in check_tf_version\n",
      "      'import tensorflow failed, is it installed?\\n\\n%s' % traceback.format_exc())\n",
      "  distutils.errors.DistutilsPlatformError: import tensorflow failed, is it installed?\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-d0wy1r6w/horovod/setup.py\", line 72, in check_tf_version\n",
      "      import tensorflow as tf\n",
      "  ModuleNotFoundError: No module named 'tensorflow'\n",
      "  \n",
      "  \n",
      "  INFO: Unable to build PyTorch plugin, will skip it.\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-d0wy1r6w/horovod/setup.py\", line 1060, in check_torch_version\n",
      "      import torch\n",
      "  ModuleNotFoundError: No module named 'torch'\n",
      "  \n",
      "  During handling of the above exception, another exception occurred:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-d0wy1r6w/horovod/setup.py\", line 1387, in build_extensions\n",
      "      torch_version = check_torch_version()\n",
      "    File \"/tmp/pip-install-d0wy1r6w/horovod/setup.py\", line 1067, in check_torch_version\n",
      "      'import torch failed, is it installed?\\n\\n%s' % traceback.format_exc())\n",
      "  distutils.errors.DistutilsPlatformError: import torch failed, is it installed?\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-d0wy1r6w/horovod/setup.py\", line 1060, in check_torch_version\n",
      "      import torch\n",
      "  ModuleNotFoundError: No module named 'torch'\n",
      "  \n",
      "  \n",
      "  INFO: Unable to build MXNet plugin, will skip it.\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-d0wy1r6w/horovod/setup.py\", line 88, in check_mx_version\n",
      "      import mxnet as mx\n",
      "  ModuleNotFoundError: No module named 'mxnet'\n",
      "  \n",
      "  During handling of the above exception, another exception occurred:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-d0wy1r6w/horovod/setup.py\", line 1403, in build_extensions\n",
      "      build_mx_extension(self, options)\n",
      "    File \"/tmp/pip-install-d0wy1r6w/horovod/setup.py\", line 1000, in build_mx_extension\n",
      "      check_mx_version()\n",
      "    File \"/tmp/pip-install-d0wy1r6w/horovod/setup.py\", line 95, in check_mx_version\n",
      "      'import mxnet failed, is it installed?\\n\\n%s' % traceback.format_exc())\n",
      "  distutils.errors.DistutilsPlatformError: import mxnet failed, is it installed?\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-d0wy1r6w/horovod/setup.py\", line 88, in check_mx_version\n",
      "      import mxnet as mx\n",
      "  ModuleNotFoundError: No module named 'mxnet'\n",
      "  \n",
      "  \n",
      "  error: None of TensorFlow, PyTorch, or MXNet plugins were built. See errors above.\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for horovod\n",
      "\u001b[0m  Running setup.py clean for horovod\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.17.1-cp36-none-any.whl size=488730 sha256=835e7a5c6dcfec08b203656e38cf8f8a0df92c115fc119592e6149c7b86c25e9\n",
      "  Stored in directory: /root/.cache/pip/wheels/0c/61/d2/d6b7317325828fbb39ee6ad559dbe4664d0896da4721bf379e\n",
      "  Building wheel for json-logging-py (setup.py): started\n",
      "  Building wheel for json-logging-py (setup.py): finished with status 'done'\n",
      "  Created wheel for json-logging-py: filename=json_logging_py-0.2-cp36-none-any.whl size=3924 sha256=4efecd147dc42a4d295af1f748765fc5f6c6be8f92764f8abcc45a73bf0580a5\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/2e/1c/c638b7589610d8b9358a6e5eb008edacb8b3e9b6d1edc9479f\n",
      "  Building wheel for psutil (setup.py): started\n",
      "  Building wheel for psutil (setup.py): finished with status 'done'\n",
      "  Created wheel for psutil: filename=psutil-5.6.7-cp36-cp36m-linux_x86_64.whl size=274794 sha256=cc55aa720a410591e8d8d63b843d877a053ef4b0ee03078826e8f2b5f6d38dc5\n",
      "  Stored in directory: /root/.cache/pip/wheels/52/41/b0/bf50409fe2b1d3b79afa3eed71b54b3e30fe5b695db2c7ba2e\n",
      "  Building wheel for pyyaml (setup.py): started\n",
      "  Building wheel for pyyaml (setup.py): finished with status 'done'\n",
      "  Created wheel for pyyaml: filename=PyYAML-5.3-cp36-cp36m-linux_x86_64.whl size=44229 sha256=9cfcc0e635b2c671b96319ab10d8c52d453108c2a41314a716641c9b01f59d60\n",
      "  Stored in directory: /root/.cache/pip/wheels/e4/76/4d/a95b8dd7b452b69e8ed4f68b69e1b55e12c9c9624dd962b191\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-cp36-none-any.whl size=121931 sha256=7e7b78ab32911668185d4e75b7bfa6b3d5c10fc2dc71d63994cb491f46b36c42\n",
      "  Stored in directory: /root/.cache/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n",
      "  Building wheel for liac-arff (setup.py): started\n",
      "  Building wheel for liac-arff (setup.py): finished with status 'done'\n",
      "  Created wheel for liac-arff: filename=liac_arff-2.4.0-cp36-none-any.whl size=13333 sha256=8fdd1472f936264983abeed5735815ca8420c5199db3c2b02e418e4178e4aa6b\n",
      "  Stored in directory: /root/.cache/pip/wheels/d1/6a/e7/529dc54d76ecede4346164a09ae3168df358945612710f5203\n",
      "  Building wheel for dill (setup.py): started\n",
      "  Building wheel for dill (setup.py): finished with status 'done'\n",
      "  Created wheel for dill: filename=dill-0.3.1.1-cp36-none-any.whl size=78532 sha256=0d40980199bfcb745508868c380b4b33e02a77e99f592e97ff92656676f7fb67\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
      "  Building wheel for pycparser (setup.py): started\n",
      "  Building wheel for pycparser (setup.py): finished with status 'done'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Created wheel for pycparser: filename=pycparser-2.19-py2.py3-none-any.whl size=111029 sha256=a503114f8c16167d315064b7ee809b4e7111cbb8226c62fd6800e1185a8d7b9b\n",
      "  Stored in directory: /root/.cache/pip/wheels/f2/9a/90/de94f8556265ddc9d9c8b271b0f63e57b26fb1d67a45564511\n",
      "Successfully built future json-logging-py psutil pyyaml absl-py liac-arff dill pycparser\n",
      "Failed to build horovod\n",
      "\u001b[91mERROR: torchvision 0.4.1 has requirement torch==1.3.0, but you'll have torch 1.3.1 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: typing-extensions, six, protobuf, numpy, onnx, Pillow, liac-arff, dill, python-dateutil, urllib3, idna, chardet, requests, pytz, PyJWT, pycparser, cffi, cryptography, adal, pandas, azureml-model-management-sdk, gunicorn, configparser, applicationinsights, json-logging-py, werkzeug, click, MarkupSafe, Jinja2, itsdangerous, flask, pyopenssl, pyasn1, ndg-httpsclient, isodate, oauthlib, requests-oauthlib, msrest, contextlib2, msrestazure, azure-common, azure-mgmt-keyvault, azure-mgmt-containerregistry, jmespath, jsonpickle, jeepney, SecretStorage, websocket-client, docker, azure-graphrbac, azure-mgmt-resource, azure-mgmt-storage, pathspec, ruamel.yaml, backports.weakref, backports.tempfile, azure-mgmt-authorization, azureml-core, azureml-defaults, torch, torchvision, cloudpickle, psutil, pyyaml, horovod, grpcio, markdown, absl-py, tensorboard, future\n",
      "\n",
      "    Running setup.py install for horovod: started\n"
     ]
    }
   ],
   "source": [
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output=True, wait_post_processing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a model trained on a remote cluster. Retrieve all the metrics logged during the run, including the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tune model hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen how to do a simple Scikit-learn training run using the SDK, let's see if we can further improve the accuracy of our model. We can optimize our model's hyperparameters using Azure Machine Learning's hyperparameter tuning capabilities.\n",
    "\n",
    "First, we will define the hyperparameter space to sweep over. Let's tune the `kernel` and `penalty` parameters. In this example we will use random sampling to try different configuration sets of hyperparameters to maximize our primary metric, `Accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.sampling import BayesianParameterSampling, RandomParameterSampling\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.parameter_expressions import uniform, loguniform\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "\n",
    "param_sampling = RandomParameterSampling({\n",
    "    '--dist-backend' : 'nccl',\n",
    "    '--dist-url': '$AZ_BATCHAI_PYTORCH_INIT_METHOD',\n",
    "    '--rank': '$AZ_BATCHAI_TASK_INDEX',\n",
    "    '--world-size': 2,\n",
    "    '--lr': loguniform(0.0005, 0.01),\n",
    "    '--momentum': uniform(0.45, 0.55),\n",
    "    '--weight-decay': uniform(1e-5, 1e-3)\n",
    "    })\n",
    "\n",
    "hyperdrive_run_config = HyperDriveConfig(estimator=est,\n",
    "                                         hyperparameter_sampling=param_sampling, \n",
    "                                         primary_metric_name='loss',\n",
    "                                         primary_metric_goal=PrimaryMetricGoal.MINIMIZE,\n",
    "                                         max_total_runs=16,\n",
    "                                         max_concurrent_runs=4,\n",
    "                                         policy=BanditPolicy(slack_factor=0.1, evaluation_interval=1, delay_evaluation=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lauch the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run = experiment.submit(hyperdrive_run_config)\n",
    "hyperdrive_run.wait_for_completion(show_output=True, wait_post_processing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times, finding the best hyperparameter values for your model can be an iterative process, needing multiple tuning runs that learn from previous hyperparameter tuning runs. Reusing knowledge from these previous runs will accelerate the hyperparameter tuning process, thereby reducing the cost of tuning the model and will potentially improve the primary metric of the resulting model. When warm starting a hyperparameter tuning experiment with Bayesian sampling, trials from the previous run will be used as prior knowledge to intelligently pick new samples, so as to improve the primary metric. Additionally, when using Random or Grid sampling, any early termination decisions will leverage metrics from the previous runs to determine poorly performing training runs. \n",
    "\n",
    "Azure Machine Learning allows you to warm start your hyperparameter tuning run by leveraging knowledge from up to 5 previously completed hyperparameter tuning parent runs. \n",
    "\n",
    "Additionally, there might be occasions when individual training runs of a hyperparameter tuning experiment are cancelled due to budget constraints or fail due to other reasons. It is now possible to resume such individual training runs from the last checkpoint (assuming your training script handles checkpoints). Resuming an individual training run will use the same hyperparameter configuration and mount the storage used for that run. The training script should accept the \"--resume-from\" argument, which contains the checkpoint or model files from which to resume the training run. You can also resume individual runs as part of an experiment that spends additional budget on hyperparameter tuning. Any additional budget, after resuming the specified training runs is used for exploring additional configurations.\n",
    "\n",
    "For more information on warm starting and resuming hyperparameter tuning runs, please refer to the [Hyperparameter Tuning for Azure Machine Learning documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all jobs finish, we can find out the one that has the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "print(best_run.get_details()['runDefinition']['arguments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Register model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in the training script wrote the file `model.pkl` in a directory named `outputs` in the VM of the cluster where the job is executed. `outputs` is a special directory in that all content in this  directory is automatically uploaded to your workspace.  This content appears in the run record in the experiment under your workspace. Hence, the model file is now also available in your workspace.\n",
    "\n",
    "You can see files associated with that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.get_file_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the model in the workspace so that you (or other collaborators) can later query, examine, and deploy this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "model = best_run.register_model(model_name='pytorch-model',\n",
    "                                model_path='outputs/model.onnx',\n",
    "                                model_framework=Model.Framework.ONNX,\n",
    "                                model_framework_version='1.0',\n",
    "                                description='PyTorch MNIST classification.',\n",
    "                                tags={'area': 'mnist', 'type': 'pytorch'})\n",
    "\n",
    "print(model.name, model.id, model.version, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your model is ready for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No-code model deployment is currently in preview and supports various frameworks and model types including Tensorflow SavedModel format, ONNX models and Scikit-learn models. No code model deployment is supported for all built-in scikit-learn model types.\n",
    "\n",
    "The deployment will take a few minutes and will take place on an Azure Container Instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "service_no_code = Model.deploy(workspace=ws,\n",
    "                               name='sample-service',\n",
    "                               models=[model])\n",
    "service_no_code.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If deployment fails, then retry with:\n",
    "# service_no_code.update(models=[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "service_no_code.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert this Webservice object into a JSON serialized dictionary, which lists all the details of the webservice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "service_no_code.serialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is an example of a Python client that can be used with the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "# Two sets of data to score, so we get two results back.\n",
    "dummy_input = torch.randn(1, 1, 28, 28, device='cpu')\n",
    "data = {'data':\n",
    "        [\n",
    "            dummy_input.tolist()\n",
    "        ]\n",
    "        }\n",
    "# Convert to JSON string.\n",
    "input_data = json.dumps(data)\n",
    "\n",
    "# Make the request and display the response.\n",
    "resp = service_no_code.run(input_data)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the service to save cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_no_code.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
