{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Azure ML Python Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.83\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Authentication and initializing Azure Machine Learning Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: myworkspace\n",
      "Azure region: westeurope\n",
      "Subscription id: 558bd446-4212-46a2-908c-9ab0a628705e\n",
      "Resource group: azure-ml-service-rg\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace=ws, name='introduction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Starting the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start logging for the run\n",
    "run = experiment.start_logging()\n",
    "\n",
    "# access the run id for use later\n",
    "run_id = run.id\n",
    "\n",
    "# change the scale factor on different runs to see how you can compare multiple runs\n",
    "scale_factor = 2\n",
    "\n",
    "# change the category on different runs to see how to organize data in reports\n",
    "category = 'Red'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a run is started you can see the run in the portal by simply typing ``run``.  Clicking on the \"Link to Portal\" link will take you to the Run Details page that shows the metrics you have logged and other run properties.  You can refresh this page after each logging statement to see the updated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also view an experiement similarly by typing `experiment`.  The portal link will take you to the experiment's Run History page that shows all runs and allows you to analyze trends across multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Log metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics are visible in the run details page in the AzureML portal and also can be analyzed in experiment reports.  The run details page looks as below and contains tabs for Details, Outputs, Logs, and Snapshot.  \n",
    "* The Details page displays attributes about the run, plus logged metrics and images.  Metrics that are vectors appear as charts.  \n",
    "* The Outputs page contains any files, such as models, you uploaded into the \"outputs\" directory from your run into storage.  If you place files in the \"outputs\" directory locally, the files are automatically uploaded on your behald when the run is completed.\n",
    "* The Logs page allows you to view any log files created by your run.  Logging runs created in notebooks typically do not generate log files.\n",
    "* The Snapshot page contains a snapshot of the directory specified in the ''start_logging'' statement, plus the notebook at the time of the ''start_logging'' call.  This snapshot and notebook can be downloaded from the Run Details page to continue or reproduce an experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell logs a string metric.  A string metric is simply a string value associated with a name.  A string metric String metrics are useful for labelling runs and to organize your data.  Typically you should log all string parameters as metrics for later analysis - even information such as paths can help to understand how individual experiements perform differently.\n",
    "\n",
    "String metrics can be used in the following ways:\n",
    "* Plot in hitograms\n",
    "* Group by indicators for numerical plots\n",
    "* Filtering runs\n",
    "\n",
    "String metrics appear in the **Tracked Metrics** section of the Run Details page and can be added as a column in Run History reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log a string metric\n",
    "run.log(name='Category', value=category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log numerical values\n",
    "run.log(name=\"scale factor\", value = scale_factor)\n",
    "run.log(name='Magic Number', value=42 * scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fibonacci_values = [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89]\n",
    "scaled_values = (i * scale_factor for i in fibonacci_values)\n",
    "\n",
    "# Log a list of values. Note this will generate a single-variable line chart.\n",
    "run.log_list(name='Fibonacci', value=scaled_values)\n",
    "\n",
    "for i in tqdm(range(-10, 10)):\n",
    "    # log a metric value repeatedly, this will generate a single-variable line chart.\n",
    "    run.log(name='Sigmoid', value=1 / (1 + np.exp(-i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to hold a table of values\n",
    "sines = {}\n",
    "sines['angle'] = []\n",
    "sines['sine'] = []\n",
    "\n",
    "for i in tqdm(range(-10, 10)):\n",
    "    angle = i / 2.0 * scale_factor\n",
    "    \n",
    "    # log a 2 (or more) values as a metric repeatedly. This will generate a 2-variable line chart if you have 2 numerical columns.\n",
    "    run.log_row(name='Cosine Wave', angle=angle, cos=np.cos(angle))\n",
    "        \n",
    "    sines['angle'].append(angle)\n",
    "    sines['sine'].append(np.sin(angle))\n",
    "\n",
    "# log a dictionary as a table, this will generate a 2-variable chart if you have 2 numerical columns\n",
    "run.log_table(name='Sine Wave', value=sines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'outputs/myfile.txt'\n",
    "\n",
    "with open(file_name, \"w\") as f:\n",
    "    f.write('This is an output file that will be uploaded.\\n')\n",
    "\n",
    "# Upload the file explicitly into artifacts \n",
    "run.upload_file(name = file_name, path_or_stream = file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete the run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `run.complete()` marks the run as completed and triggers the output file collection.  If for any reason you need to indicate the run failed or simply need to cancel the run you can call `run.fail()` or `run.cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FOLDER_NAME = 'train'\n",
    "TRAIN_FILE_NAME = 'train.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join(\".\", TRAIN_FOLDER_NAME), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $TRAIN_FOLDER_NAME/$TRAIN_FILE_NAME\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "submitted_run = Run.get_context()\n",
    "submitted_run.log(name=\"message\", value=\"Hello from run!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Estimator(source_directory=TRAIN_FOLDER_NAME, \n",
    "                entry_script=TRAIN_FILE_NAME,\n",
    "                compute_target='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can refresh the run in the Azure portal to see all of your results.  In many cases you will want to analyze runs that were performed previously to inspect the contents or compare results.  Runs can be fetched from their parent Experiment object using the ``Run()`` constructor or the ``experiment.get_runs()`` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_run = Run(experiment, run_id)\n",
    "fetched_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call ``run.get_metrics()`` to retrieve all the metrics from a run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the files uploaded for this run by calling ``run.get_file_names()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_run.get_file_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you know the file names in a run, you can download the files using the ``run.download_file()`` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('files', exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    dest = os.path.join('files', f.split('/')[-1])\n",
    "    print('Downloading file {} to {}...'.format(f, dest))\n",
    "    fetched_run.download_file(f, dest)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often when you analyze the results of a run, you may need to tag that run with important personal or external information.  You can add a tag to a run using the ``run.tag()`` method.  AzureML supports valueless and valued tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_run.tag(\"My Favorite Run\")\n",
    "fetched_run.tag(\"Competition Rank\", 1)\n",
    "\n",
    "fetched_run.get_tags()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
