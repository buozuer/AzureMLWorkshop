{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Time Series Anomaly Detection with Auto ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand/auto-ml-forecasting-energy-demand.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this challenge you will get familar with [AutoML SDK](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train) and use it's [timeseries forecasting](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-forecast) capabilities. We will use the forecasting capability to predict anomalies on a timeseries data. Relevant links will provided in the Notebook and help you to solve the tasks.\n",
    "\n",
    "Generally a very good source of information is the [Python SDK reference](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py) for Azure Machine learning.\n",
    "\n",
    "**Note:** In the Notebook altough there is no assigned tasks, you may see **<font color=red>???</font>** in various code blocks. Please fill them in, to successfully continue runnign the notebook. The purpose of those are to keep you engaging with the notebook and help you in learning the main [AutoML SDK](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train) commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Azure ML Python Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If the SDK verison is lower than the latest (1.12.0) please upgrade you AzureML SDK:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade azureml-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Authentication and initializing Azure Machine Learning Workspace\n",
    "As a first step you have to authenticate against the Azure [Machine Learning Workspace](https://ml.azure.com/). This can be achieved in different ways:\n",
    "\n",
    "1. **Interactive Login Authentication:** The interactive authentication is suitable for local experimentation on your own computer.\n",
    "2. **Azure CLI Authentication:** Azure CLI authentication is suitable if you are already using Azure CLI for managing Azure resources, and want to sign in only once.\n",
    "3. **Managed Service Identity (MSI) Authentication:** The MSI authentication is suitable for automated workflows, for example as part of Azure Devops build.\n",
    "4. **Service Principal Authentication:** The Service Principal authentication is suitable for automated workflows, for example as part of Azure Devops build.\n",
    "\n",
    "For now, we will use the interactive authentication, which is the default mode when using Azure ML SDK. When you connect to your workspace using `Workspace.from_config`, you will get an interactive login dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.???()\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the user you're authenticated as must have access to the subscription and resource group. If you receive an error\n",
    "```\n",
    "AuthenticationException: You don't have access to xxxxxx-xxxx-xxx-xxx-xxxxxxxxxx subscription. All the subscriptions that you have access to = ...\n",
    "```\n",
    "check that the you used correct login and entered the correct subscription ID.\n",
    "\n",
    "Alternatively, if you are using on premises Jupyter Notebooks you can also specify the details of your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Alternative login method\n",
    "\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "\n",
    "ws = Workspace(\n",
    "    subscription_id=\"<your-subscription-id>\",\n",
    "    resource_group=\"<your-resource-group-name>\",\n",
    "    workspace_name=\"<your-workspace-name>\",\n",
    "    auth=interactive_auth\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already created an Azure ML `Workspace` object. For running Automated ML jobs you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a name for the run history container in the workspace\n",
    "from azureml.core import ???\n",
    "experiment_name = 'ch5-forecast-anomaly-automl'\n",
    "\n",
    "experiment = Experiment(\n",
    "    workspace=ws,\n",
    "    name=experiment_name\n",
    ")\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. About  the NYC Energy Data\n",
    "\n",
    "For this challenge we will be working with the  [Energy Demand data from New York City](http://mis.nyiso.com/public/P-58Blist.htm). We preprocessed the data for you, cleansed the mising information (The NYC Energy dataset is originally missing energy demand values for all datetimes later than August 10th, 2017 5AM), and seperated the data into test and train sets and added some obvious anomalies to the test set for the sake of visibility. \n",
    "\n",
    "The data is stored under the `dataset` directory in the `challenge_4` folder as two seperate csv's.\n",
    "\n",
    "- **nyc_energy_train.csv**: containes the training data\n",
    "- **nyc_energy_test.csv**: containes the test data with 2 artifical major and several original minor anomalies\n",
    "\n",
    "Here is what we need to know about the dataset.\n",
    "\n",
    "- **demand column** contains the Energy Demand information and it is what we want to forecast and detect anomalies. We will be marking this column as **target_column_name** . \n",
    "\n",
    "- **timeStamp column**  is the time axis of the data. Along this time axis we will forecast and detect anomalies.  We will be marking this column as **time_column_name** .\n",
    "\n",
    "The other columns, **temp** and **precip**, are implicitly designated as features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = \"demand\"\n",
    "time_column_name = \"timeStamp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's continue with the process of loading the data to the default datastore and registering their datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Upload and Register the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every workspace comes with a default [datastore](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data) (and you can register more) which is backed by the Azure blob storage account associated with the workspace. We can use it to transfer data from local to the cloud, and create Dataset from it. We will now upload the Iris data to the default datastore (blob) within your workspace.\n",
    "\n",
    "By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all datastores registered in the current workspace\n",
    "datastores = ws.???\n",
    "for name, datastore in datastores.???():\n",
    "    print(name, datastore.datastore_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge we will use the [default datastore](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data#get-datastores-from-your-workspace) that comes with the Azure Machine Learning Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default datastore\n",
    "datastore = ws.???()\n",
    "\n",
    "print(datastore.name,\n",
    "      datastore.datastore_type,\n",
    "      datastore.account_name,\n",
    "      datastore.container_name, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** upload the datatsets and create two tabular datasets suitable to use in time series operations. Fill in the missing code marked with **<font color='red'>???</font>** \n",
    "\n",
    "**Hint:** Make sure that the timeseries columns are acknowledged while loading the datasets. Check: [tabular dataset](https://docs.microsoft.com/bs-latn-ba/azure/machine-learning/service/how-to-create-register-datasets#dataset-types) ,[AutoML dataset methods](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py#methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Upload the data to the default datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore.???(\n",
    "    files=[\"./dataset/nyc_energy_train.csv\"],\n",
    "    ???=\"train_dataset/nyc_energy/\",\n",
    "    overwrite=True,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "datastore.???(\n",
    "    files=[\"./dataset/nyc_energy_test.csv\"],\n",
    "    ???=\"test_dataset/nyc_energy/\",\n",
    "    overwrite=True,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, Create a Tabular Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "train_dataset = Dataset.???.???(\n",
    "    path=[(datastore, \"train_dataset/nyc_energy/nyc_energy_train.csv\")],\n",
    "    validate=True,\n",
    "    include_path=False,\n",
    "    infer_column_types=True,\n",
    "    set_column_types=None,\n",
    "    separator=\",\",\n",
    "    header=True,\n",
    "    partition_format=None,\n",
    "    support_multi_line=False,\n",
    "    empty_as_string=False\n",
    ").with_timestamp_columns(fine_grain_timestamp=time_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "test_dataset = Dataset.???.???(\n",
    "    path=[(datastore, \"test_dataset/nyc_energy/nyc_energy_test.csv\")],\n",
    "    validate=True,\n",
    "    include_path=False,\n",
    "    infer_column_types=True,\n",
    "    set_column_types=None,\n",
    "    separator=\",\",\n",
    "    header=True,\n",
    "    partition_format=None,\n",
    "    support_multi_line=False,\n",
    "    empty_as_string=False\n",
    ").with_timestamp_columns(fine_grain_timestamp=time_column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally: Register The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.???(\n",
    "    workspace=ws,\n",
    "    name=\"nyc_energy_train\",\n",
    "    description=\"NYC Energy Demand Time Series Data Train Set\",\n",
    "    create_new_version=True\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.???(\n",
    "    workspace=ws,\n",
    "    name=\"nyc_energy_test\",\n",
    "    description=\"NYC Energy Demand Time Series Data Test Set\",\n",
    "    create_new_version=True\n",
    ")\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we uploaded our data we can start building our compute instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create or Attach existing AmlCompute\n",
    "A compute target is required to execute a remote Automated ML run. \n",
    "\n",
    "[Azure Machine Learning Compute](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute) is a managed-compute infrastructure that allows the user to easily create a single or multi-node compute. In this tutorial, you create AmlCompute as your training compute resource.\n",
    "\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota.\n",
    "\n",
    "**<font color=green> Here since the analysis we are running are more compute intensive, the compute clusters used on the previous challenges may not be performant and can take a longer time to train the models. therefore we are creating a cluster with a more superior Azure SKU</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ???, ???\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"ch5cpucluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ???(\n",
    "        workspace=ws,\n",
    "        name=cluster_name\n",
    "    )\n",
    "    print(\"Found existing compute target\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_DS13_V2\",\n",
    "        vm_priority=\"dedicated\",\n",
    "        min_nodes=0,\n",
    "        max_nodes=6,\n",
    "        idle_seconds_before_scaledown=1800\n",
    "    )\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(\n",
    "        workspace=ws,\n",
    "        name=cluster_name,\n",
    "        provisioning_configuration=compute_config\n",
    "    )\n",
    "\n",
    "    # wait until the cluster has been provisioned\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.???(\n",
    "        show_output=True,\n",
    "        min_node_count=None,\n",
    "        timeout_in_minutes=20\n",
    "    )\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time Series Operations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.  Forecasting Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting the data and evaluating the deviation of the actuals from the forecast is a widely used technique to detect anomalies in Time Seried datasets. \n",
    "\n",
    "For this challenge we will utilise [AutoML's time series forecasting capabilities](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-forecast) to detect anomalies in NY Energy dataset.\n",
    "\n",
    "Let's go through the necessary parameters that needs to be defined to male an Auto ML time series forecasting call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To define forecasting parameters for your experiment training, you can leverage the ForecastingParameters class. The table below details the forecasting parameter we will be passing into our experiment.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**time_column_name**|The name of your time column.|\n",
    "|**forecast_horizon**|The forecast horizon is how many periods forward you would like to forecast. This integer horizon is in units of the timeseries frequency (e.g. daily, weekly).|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **forecast horizon** is the number of periods into the future that the model should predict. It is generally recommended that users set forecast horizons to less than 100 time periods (i.e. less than 100 hours in the NYC energy example). Furthermore, **AutoML's memory use and computation time increase in proportion to the length of the horizon**, so consider carefully how this value is set. If a long horizon forecast really is necessary, consider aggregating the series to a coarser time scale. \n",
    "\n",
    "Learn more about forecast horizons in our [Auto-train a time-series forecast model](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-auto-train-forecast#configure-and-run-experiment) guide.\n",
    "\n",
    "In this example, we will set the horizon to 48 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Automl Configuration\n",
    "\n",
    "Instantiate an AutoMLConfig object. This config defines the settings and data used to run the experiment. We can provide extra configurations within 'automl_settings', for this forecasting task we add the forecasting parameters to hold all the additional forecasting parameters.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>|\n",
    "|**blocked_models**|Models in blocked_models won't be used by AutoML. All supported models can be found at [here](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.constants.supportedmodels.forecasting?view=azure-ml-py).|\n",
    "|**experiment_timeout_hours**|Maximum amount of time in hours that the experiment take before it terminates.|\n",
    "|**training_data**|The training data to be used within the experiment.|\n",
    "|**label_column_name**|The name of the label column.|\n",
    "|**compute_target**|The remote compute for training.|\n",
    "|**n_cross_validations**|Number of cross validation splits. Rolling Origin Validation is used to split time-series in a temporally consistent way.|\n",
    "|**enable_early_stopping**|Flag to enble early termination if the score is not improving in the short term.|\n",
    "|**forecasting_parameters**|A class holds all the forecasting related parameters.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Instantiate an AutoMLConfig object for the forecasting purpose of our training dataset.\n",
    "\n",
    "**Hint 1:** [AutoML forecast configuration settings](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-forecast#configuration-settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint 2:** Using the **blocked_models** parameter to exclude some models like ***'ExtremeRandomTrees', 'AutoArima', 'Prophet'*** that take a longer time to train on this dataset, this can improve your run durations. You can choose to remove models from the blocked_models list but you may need to increase the experiment_timeout_hours parameter value to get results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl import ???\n",
    "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
    "\n",
    "forecasting_parameters = ForecastingParameters(\n",
    "    time_column_name=time_column_name, \n",
    "    forecast_horizon=???\n",
    ")\n",
    "\n",
    "automl_config = ???(\n",
    "    task=\"forecasting\",                             \n",
    "    primary_metric=\"normalized_root_mean_squared_error\",  # \"normalized_mean_absolute_error\"\n",
    "    ??? = [\"ExtremeRandomTrees\", \"AutoArima\", \"Prophet\"],\n",
    "    experiment_timeout_hours=1,\n",
    "    training_data=???,\n",
    "    label_column_name=???,\n",
    "    compute_target=compute_target,\n",
    "    enable_early_stopping=True,\n",
    "    n_cross_validations=3,                             \n",
    "    verbosity=logging.INFO,\n",
    "    max_cores_per_iteration=-1,\n",
    "    max_concurrent_iterations=6,\n",
    "    forecasting_parameters=forecasting_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You dont need to create a training script to train an AutoML model. You can directly call the `submit` method on the experiment object and pass the run configuration. Depending on the data and the number of iterations this can run for a while.\n",
    "\n",
    "**Task:** Call the `submit` method on the experiment object and pass the run configuration.\n",
    "\n",
    "**Hint:** https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment(class)?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.???(automl_config)\n",
    "remote_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task:** View the the `remote_run` object and call the required methods to follow the status.\n",
    "\n",
    "**Hint:** https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run(class)?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.???(\n",
    "    ???=True,\n",
    "    wait_post_processing=True\n",
    ")"
   ]
  },
  {
   "source": [
    "**TASK:** Please go back to the run in the portal and look at the model explanation for the best model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Retrieve the Best Model\n",
    "**Task:** Select the best model from all the training iterations using get_output method of `remote_run` object.\n",
    "\n",
    "Hint: https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.run.automlrun?view=azure-ml-py#get-output-iteration--typing-union-int--nonetype----none--metric--typing-union-str--nonetype----none--return-onnx-model--bool---false--return-split-onnx-model--typing-union-azureml-automl-core-onnx-convert-onnx-convert-constants-splitonnxmodelname--nonetype----none----kwargs--typing-any-----typing-tuple-azureml-core-run-run--typing-any-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.???()\n",
    "fitted_model.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Forecasting\n",
    "\n",
    "Now that we have retrieved the best pipeline/model, it can be used to make predictions on test data. First, we remove the target values from the test set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_dataset.???().reset_index(drop=True)\n",
    "y_test = X_test.pop(target_column_name).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_run = experiment.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Forecast Function\n",
    "For forecasting, we will use the forecast function instead of the predict function. Using the predict method would result in getting predictions for EVERY horizon the forecaster can predict at. This is useful when training and evaluating the performance of the forecaster at various horizons, but the level of detail is excessive for normal use. Forecast function also can handle more complicated scenarios, see the [forecast function notebook](../forecasting-forecast-function/auto-ml-forecasting-function.ipynb).\n",
    "\n",
    "**Task:** Forecast the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The featurized data, aligned to y, will also be returned.\n",
    "# This contains the assumptions that were made in the forecast\n",
    "# and helps align the forecast to the original data\n",
    "y_predictions, X_trans = fitted_model.???(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Evaluate\n",
    "To evaluate the accuracy of the forecast, we'll compare against the actual sales quantities for some select metrics, included the mean absolute percentage error (MAPE).\n",
    "\n",
    "It is a good practice to always align the output explicitly to the input, as the count and order of the rows may have changed during transformations that span multiple rows.\n",
    "\n",
    "Below we are using the forecasting_helper.py existing in the same directory with our notebook to combine the datasets easly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecasting_helper  import align_outputs \n",
    "\n",
    "df_all = align_outputs(y_predictions, X_trans, X_test, y_test, target_column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Retrieve Scores for the best model\n",
    "\n",
    "**Hint:** [azureml.automl.runtime.shared.score.scoring](https://docs.microsoft.com/en-us/python/api/azureml-automl-runtime/azureml.automl.runtime.shared.score.scoring?view=azure-ml-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core.shared import constants\n",
    "from azureml.automl.runtime.shared.score import ???\n",
    "\n",
    "\n",
    "# use automl metrics module\n",
    "scores = scoring.score_regression(\n",
    "    y_test = df_all[target_column_name],\n",
    "    y_pred = df_all[\"predicted\"],\n",
    "    metrics=list(constants.Metric.SCALAR_REGRESSION_SET))\n",
    "\n",
    "print(\"[Test data scores]\")\n",
    "for key, value in scores.items():    \n",
    "    print(f\"{key}:  {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Plot the test sets Actuals and Predictions to visualy examine the datasets. ick your anomally candidates from the Actual data.\n",
    "\n",
    "Hint: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run(class)?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot\n",
    "figsize=(12, 7)\n",
    "plt.figure(figsize=figsize)\n",
    "plt.plot(df_all[target_column_name], color=\"green\", label=\"Actuals\")\n",
    "plt.plot(df_all[\"predicted\"], color=\"blue\", label=\"Predicted\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "local_run.???(name=\"Graph of Aztuals vs Predicted Data\", ???=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Detect the anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect the anomalies we need to decide what our normal and anormal date looks like. There are several ways to identify these boundaries.\n",
    "\n",
    " a. Check whether the absolute error is bigger than [MAPE](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error)\n",
    " \n",
    " b. Calculate a moving average of the error check wheter the actual error is within\n",
    " \n",
    " c. Adjust the moving average with dataset median error to reduce the effect of anomalies in the normal data boundry.\n",
    " \n",
    " Below function does the item c.  run ad check the results.\n",
    " \n",
    " **Bonus Task:** Implement MAPE techique with the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def detect_classify_anomalies(df,target_column_name,window,med_abs_error):\n",
    "    def check_boundary (x,ub,lb):\n",
    "        if (x > ub or x < lb):\n",
    "            val = x\n",
    "        else:\n",
    "            val = np.nan\n",
    "        return val\n",
    "   \n",
    "    df[\"abserror\"] = abs(df[target_column_name]-df[\"predicted\"])\n",
    "    df[\"rel_error\"] = (df[\"abserror\"].rolling(window = window).sum().fillna(med_abs_error) + med_abs_error) / (window + 1)\n",
    "    df[\"upper\"] = (df[\"predicted\"] + df[\"rel_error\"] * 1.5)\n",
    "    df[\"lower\"] = (df[\"predicted\"] - df[\"rel_error\"] * 1.5)\n",
    "    df[\"anomaly\"] = df.apply(lambda x: check_boundary(x[target_column_name], x[\"upper\"], x[\"lower\"]), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = ???(df_all, target_column_name, 4, scores[\"median_absolute_error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Plot the test sets Actuals , Predictions,Anomaly boundaries and anomalies  to visualy examine the datasets. Check whether the anomalies you visiauly picked at the previous excersize are marked as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "figsize=(12, 7)\n",
    "plt.figure(figsize=figsize)\n",
    "plt.plot(df_all[target_column_name], color=\"green\", label=\"Actuals\")\n",
    "plt.plot(df_all[\"upper\"], color=\"gray\", linestyle=\"dashed\", label=\"Upper Anomaly Limit\")\n",
    "plt.plot(df_all[\"lower\"], color=\"gray\", linestyle=\"dashed\", label=\"Lower Anomaly Limit\")\n",
    "plt.plot(df_all[???], color=\"blue\",label=\"Predicted\")\n",
    "plt.plot(df_all[???] , color=\"red\", marker=\"o\",label=\"Anomaly\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "local_run.???(name=\"Graph of Actuals vs Predicted with Anomalies and Anomaly Boundaries\", plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this challenge we performed some tasks locally and manually and some other tasks remotely via AutoML. on the next challenge you will see how you can create pipelines to orchestrate and manage these seperate tasks**"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "erwright"
   }
  ],
  "categories": [
   "how-to-use-azureml",
   "automated-machine-learning"
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}